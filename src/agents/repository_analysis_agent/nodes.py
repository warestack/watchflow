from typing import Any

import httpx
import openai
import pydantic
import structlog
from langchain_core.messages import HumanMessage, SystemMessage

from src.agents.repository_analysis_agent.models import AnalysisState, PRSignal, RuleRecommendation
from src.agents.repository_analysis_agent.prompts import REPOSITORY_ANALYSIS_SYSTEM_PROMPT, RULE_GENERATION_USER_PROMPT
from src.integrations.github.api import github_client
from src.integrations.providers.factory import get_chat_model

logger = structlog.get_logger()


def _map_github_pr_to_signal(pr_data: dict[str, Any]) -> PRSignal:
    """
    Convert raw GitHub API PR response into a PRSignal Pydantic model.

    This helper implements the AI detection heuristic for the Immune System (Phase 6).
    It flags PRs as potentially AI-generated based on common LLM tool signatures in
    the description or title.

    Args:
        pr_data: Dictionary from GitHub API with keys: number, title, body,
                 author_association, lines_changed, has_issue_ref

    Returns:
        PRSignal model with all fields populated for hygiene analysis
    """
    # AI Detection Heuristic: Check for common LLM tool signatures
    body = (pr_data.get("body") or "").lower()
    title = (pr_data.get("title") or "").lower()

    ai_keywords = [
        "generated by claude",
        "cursor",
        "copilot",
        "chatgpt",
        "ai-generated",
        "llm",
        "i am an ai",
        "as an ai",
    ]

    is_ai_generated = any(keyword in body or keyword in title for keyword in ai_keywords)

    return PRSignal(
        pr_number=pr_data["number"],
        has_linked_issue=pr_data.get("has_issue_ref", False),
        author_association=pr_data.get("author_association", "NONE"),
        is_ai_generated_hint=is_ai_generated,
        lines_changed=pr_data.get("lines_changed", 0),
    )


async def fetch_repository_metadata(state: AnalysisState) -> AnalysisState:
    """
    Step 1: Gather raw signals from GitHub (Public or Private).
    This node populates the 'Shared Memory' (State) with facts about the repo.
    """
    repo = state.repo_full_name
    if not repo:
        raise ValueError("Repository full name is missing in state.")

    logger.info("repository_metadata_fetch_started", repo=repo)

    # 1. Fetch File Tree (Root)
    try:
        files = await github_client.list_directory_any_auth(repo_full_name=repo, path="")
    except httpx.HTTPStatusError as e:
        logger.error(
            "file_tree_fetch_failed",
            repo=repo,
            error=str(e),
            status_code=e.response.status_code,
            error_type="network_error",
        )
        files = []
    except Exception as e:
        logger.error("file_tree_fetch_failed", repo=repo, error=str(e), error_type="unknown_error")
        files = []

    file_names = [f["name"] for f in files] if files else []
    state.file_tree = file_names

    # 2. Heuristic Language Detection
    languages = []
    if "pom.xml" in file_names:
        languages.append("Java")
    if "package.json" in file_names:
        languages.append("JavaScript/TypeScript")
    if "requirements.txt" in file_names or "pyproject.toml" in file_names:
        languages.append("Python")
    if "go.mod" in file_names:
        languages.append("Go")
    if "Cargo.toml" in file_names:
        languages.append("Rust")
    state.detected_languages = languages

    # 3. Check for CI/CD presence
    state.has_ci = ".github" in file_names

    # 4. Fetch Documentation Snippets (for Context)
    readme_content = ""
    target_files = ["README.md", "readme.md", "CONTRIBUTING.md"]
    for target in target_files:
        if target in file_names:
            try:
                content = await github_client.get_file_content(
                    repo_full_name=repo, file_path=target, installation_id=None
                )
                if content:
                    readme_content = content[:2000]
                    break
            except httpx.HTTPStatusError:
                continue  # File not found is not a critical error
    state.readme_content = readme_content

    # 5. CODEOWNERS detection (root, .github/, docs/)
    codeowners_paths = ["CODEOWNERS", ".github/CODEOWNERS", "docs/CODEOWNERS"]
    has_codeowners = False
    for copath in codeowners_paths:
        try:
            co_content = await github_client.get_file_content(
                repo_full_name=repo, file_path=copath, installation_id=None
            )
            if co_content and len(co_content.strip()) > 0:
                has_codeowners = True
                break
        except httpx.HTTPStatusError:
            continue  # Not finding a CODEOWNERS file is expected
    state.has_codeowners = has_codeowners

    # 6. Analyze workflows for CI patterns
    workflow_patterns = []
    try:
        workflow_files = await github_client.list_directory_any_auth(repo_full_name=repo, path=".github/workflows")
        for wf in workflow_files:
            wf_name = wf["name"]
            if wf_name.endswith(".yml") or wf_name.endswith(".yaml"):
                try:
                    content = await github_client.get_file_content(
                        repo_full_name=repo, file_path=f".github/workflows/{wf_name}", installation_id=None
                    )
                    if content:
                        if "pytest" in content:
                            workflow_patterns.append("pytest")
                        if "actions/checkout" in content:
                            workflow_patterns.append("actions/checkout")
                        if "deploy" in content:
                            workflow_patterns.append("deploy")
                except httpx.HTTPStatusError:
                    continue  # A single broken workflow file shouldn't stop analysis
    except httpx.HTTPStatusError as e:
        logger.warning(
            "workflow_analysis_failed",
            repo=repo,
            error=str(e),
            status_code=e.response.status_code,
            error_type="network_error",
        )
    state.workflow_patterns = workflow_patterns

    logger.info(
        "repository_metadata_fetch_completed",
        repo=repo,
        file_count=len(file_names),
        detected_languages=languages,
        has_codeowners=has_codeowners,
        workflow_patterns=workflow_patterns,
    )

    return state


async def fetch_pr_signals(state: AnalysisState) -> AnalysisState:
    """
    Step 2: Fetch historical PR data for hygiene analysis (AI Immune System).

    This node acts as the "Sensory Input" for detecting AI spam patterns.
    It calculates HygieneMetrics from recent merged PRs to inform rule generation.
    """
    from src.agents.repository_analysis_agent.models import HygieneMetrics

    repo = state.repo_full_name
    if not repo:
        raise ValueError("Repository full name is missing in state.")

    logger.info("pr_signals_fetch_started", repo=repo)

    try:
        # Fetch recent merged PRs (last 30)
        pr_data_list = await github_client.fetch_recent_pull_requests(
            repo_full_name=repo,
            installation_id=None,  # Public repo access for now
            limit=30,
        )

        if not pr_data_list:
            # New repo or no PRs - set default metrics to avoid LLM crash
            logger.warning(
                "pr_signals_no_data", repo=repo, message="No merged PRs found. Using default hygiene metrics."
            )
            state.hygiene_summary = HygieneMetrics(
                unlinked_issue_rate=0.0, average_pr_size=0, first_time_contributor_count=0
            )
            return state

        # Convert raw PR data to PRSignal models
        pr_signals = [_map_github_pr_to_signal(pr) for pr in pr_data_list]
        state.pr_signals = pr_signals

        # Calculate HygieneMetrics
        total_prs = len(pr_signals)
        unlinked_count = sum(1 for pr in pr_signals if not pr.has_linked_issue)
        unlinked_rate = unlinked_count / total_prs if total_prs > 0 else 0.0

        avg_pr_size = sum(pr.lines_changed for pr in pr_signals) // total_prs if total_prs > 0 else 0

        first_timers = sum(1 for pr in pr_signals if pr.author_association in ["FIRST_TIME_CONTRIBUTOR", "NONE"])

        state.hygiene_summary = HygieneMetrics(
            unlinked_issue_rate=unlinked_rate, average_pr_size=avg_pr_size, first_time_contributor_count=first_timers
        )

        logger.info(
            "pr_signals_fetch_completed",
            repo=repo,
            total_prs=total_prs,
            unlinked_rate=f"{unlinked_rate:.2%}",
            avg_size=avg_pr_size,
            first_timers=first_timers,
        )

        return state

    except httpx.HTTPStatusError as e:
        logger.error(
            "pr_signals_fetch_failed",
            repo=repo,
            status_code=e.response.status_code,
            error_type="network_error",
            error=str(e),
        )
        # Set defaults on error
        state.hygiene_summary = HygieneMetrics(
            unlinked_issue_rate=0.0, average_pr_size=0, first_time_contributor_count=0
        )
        return state
    except Exception as e:
        logger.error("pr_signals_fetch_failed", repo=repo, error_type="unknown_error", error=str(e))
        # Set defaults on error
        state.hygiene_summary = HygieneMetrics(
            unlinked_issue_rate=0.0, average_pr_size=0, first_time_contributor_count=0
        )
        return state


async def generate_rule_recommendations(state: AnalysisState) -> AnalysisState:
    """
    Step 3: Send gathered signals to LLM to generate governance rules with AI Immune System reasoning.
    """
    repo_name = state.repo_full_name or "unknown/repo"
    logger.info("rule_generation_started", repo=repo_name, agent="repo_analysis")

    languages = state.detected_languages
    has_ci = state.has_ci
    has_codeowners = state.has_codeowners
    file_tree = state.file_tree
    readme_content = state.readme_content or ""
    workflow_patterns = state.workflow_patterns
    hygiene_summary = state.hygiene_summary

    # Format hygiene summary for LLM
    if hygiene_summary:
        hygiene_text = f"""- Unlinked Issue Rate: {hygiene_summary.unlinked_issue_rate:.1%} ({int(hygiene_summary.unlinked_issue_rate * 100)}% of PRs lack issue references)
- Average PR Size: {hygiene_summary.average_pr_size} lines changed
- First-Time Contributors: {hygiene_summary.first_time_contributor_count} in last 30 PRs"""
    else:
        hygiene_text = "- No PR history available (new repository or no merged PRs)"

    # 1. Construct Prompt with all available signals
    user_prompt = RULE_GENERATION_USER_PROMPT.format(
        repo_name=repo_name,
        languages=", ".join(languages) if languages else "Unknown",
        has_ci=str(has_ci),
        has_codeowners=str(has_codeowners),
        file_count=len(file_tree),
        workflow_patterns=", ".join(workflow_patterns) if workflow_patterns else "None detected",
        hygiene_summary=hygiene_text,
        file_tree_snippet="\n".join(file_tree[:25]),
        docs_snippet=readme_content[:1000],
    )

    # 2. Initialize LLM with temperature tuning for reasoning
    try:
        llm = get_chat_model(agent="repository_analysis", temperature=0.4)

        class RecommendationsList(pydantic.BaseModel):
            recommendations: list[RuleRecommendation]

        structured_llm = llm.with_structured_output(RecommendationsList)

        response = await structured_llm.ainvoke(
            [SystemMessage(content=REPOSITORY_ANALYSIS_SYSTEM_PROMPT), HumanMessage(content=user_prompt)]
        )

        valid_recs = response.recommendations
        logger.info("rule_generation_succeeded", repo=repo_name, recommendation_count=len(valid_recs))
        state.recommendations = valid_recs
        return state

    except openai.OpenAIError as e:
        logger.error("rule_generation_failed", repo=repo_name, error=str(e), error_type="llm_provider_error")
        fallback_reason = f"AI provider error: {e.__class__.__name__}"
    except pydantic.ValidationError as e:
        logger.error("rule_generation_failed", repo=repo_name, error=str(e), error_type="schema_mismatch")
        fallback_reason = "AI model returned data in an unexpected format."
    except Exception as e:
        logger.error("rule_generation_failed", repo=repo_name, error=str(e), error_type="unknown_error", exc_info=True)
        fallback_reason = f"An unexpected error occurred: {str(e)}"

    # Fallback for any of the caught exceptions
    fallback_rule = RuleRecommendation(
        key="manual_review_required",
        name="Manual Governance Review",
        description="AI analysis could not complete. Please review repository manually.",
        severity="low",
        category="system",
        reasoning=fallback_reason,
    )
    state.recommendations = [fallback_rule]
    state.error = fallback_reason
    return state
