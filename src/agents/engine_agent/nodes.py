"""
LangGraph nodes for the Rule Engine Agent with hybrid validation strategy.
"""

import asyncio
import logging
import time
from typing import Any

from langchain_core.messages import HumanMessage, SystemMessage
from src.core.ai import get_chat_model

from src.agents.engine_agent.models import (
    EngineState,
    HowToFixResponse,
    LLMEvaluationResponse,
    RuleDescription,
    StrategySelectionResponse,
    ValidationStrategy,
)
from src.agents.engine_agent.prompts import (
    create_how_to_fix_prompt,
    create_llm_evaluation_prompt,
    create_validation_strategy_prompt,
    get_llm_evaluation_system_prompt,
)
from src.core.config import config
from src.rules.validators import VALIDATOR_REGISTRY

logger = logging.getLogger(__name__)


async def analyze_rule_descriptions(state: EngineState) -> EngineState:
    """
    Analyze rule descriptions and parameters to understand rule requirements.
    """
    start_time = time.time()

    try:
        logger.info(f"üîç Analyzing {len(state.rule_descriptions)} rule descriptions")

        # Filter rules applicable to this event type
        applicable_rules = []
        for rule_desc in state.rule_descriptions:
            if state.event_type in rule_desc.event_types:
                applicable_rules.append(rule_desc)
                logger.info(f"üîç Rule '{rule_desc.description[:50]}...' is applicable to {state.event_type}")
            else:
                logger.info(
                    f"üîç Rule '{rule_desc.description[:50]}...' is NOT applicable (expects: {rule_desc.event_types})"
                )

        state.rule_descriptions = applicable_rules
        state.analysis_steps.append(f"Found {len(applicable_rules)} applicable rules out of {len(state.rules)} total")

        analysis_time = (time.time() - start_time) * 1000
        logger.info(f"üîç Rule analysis completed in {analysis_time:.2f}ms")

    except Exception as e:
        logger.error(f"‚ùå Error in rule analysis: {e}")
        state.analysis_steps.append(f"Error in rule analysis: {str(e)}")

    return state


async def select_validation_strategy(state: EngineState) -> EngineState:
    """
    Use LLM to select the best validation strategy for each rule based on available validators.
    """
    start_time = time.time()

    try:
        logger.info(f"üéØ Selecting validation strategies for {len(state.rule_descriptions)} rules using LLM")

        # Use LLM to analyze rules and select validation strategies
        llm = get_chat_model(max_tokens=2000, temperature=0.1)

        for rule_desc in state.rule_descriptions:
            # Create prompt for strategy selection
            strategy_prompt = create_validation_strategy_prompt(rule_desc, state.available_validators)
            messages = [
                SystemMessage(content=strategy_prompt),
                HumanMessage(content="Select the best validation strategy for this rule."),
            ]

            try:
                # Use structured output for reliable parsing
                structured_llm = llm.with_structured_output(StrategySelectionResponse)
                strategy_result = await structured_llm.ainvoke(messages)

                rule_desc.validation_strategy = strategy_result.strategy
                rule_desc.validator_name = strategy_result.validator_name

                logger.info(f"üéØ Rule '{rule_desc.description[:50]}...' using {rule_desc.validation_strategy} strategy")
                if rule_desc.validator_name:
                    logger.info(f"üéØ Selected validator: {rule_desc.validator_name}")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è LLM strategy selection failed for rule '{rule_desc.description[:50]}...': {e}")
                rule_desc.validation_strategy = ValidationStrategy.HYBRID
                rule_desc.validator_name = None

        strategy_time = (time.time() - start_time) * 1000
        logger.info(f"üéØ Strategy selection completed in {strategy_time:.2f}ms")

    except Exception as e:
        logger.error(f"‚ùå Error in validation strategy selection: {e}")
        state.analysis_steps.append(f"Error in strategy selection: {str(e)}")

    return state


async def execute_validator_evaluation(state: EngineState) -> EngineState:
    """
    Execute fast validator evaluations for rules that can use validators.
    """
    start_time = time.time()

    try:
        validator_rules = [
            rd for rd in state.rule_descriptions if rd.validation_strategy == ValidationStrategy.VALIDATOR
        ]
        logger.info(f"‚ö° Executing {len(validator_rules)} validator evaluations")

        if not validator_rules:
            logger.info("‚ö° No validator rules to evaluate")
            return state

        # Execute validators concurrently
        validator_tasks = []
        for rule_desc in validator_rules:
            if rule_desc.validator_name and rule_desc.validator_name in VALIDATOR_REGISTRY:
                task = _execute_single_validator(rule_desc, state.event_data)
                validator_tasks.append(task)

        if validator_tasks:
            results = await asyncio.gather(*validator_tasks, return_exceptions=True)

            # Process results
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"‚ùå Validator failed for rule '{validator_rules[i].description[:50]}...': {result}")
                    # Fallback to LLM if validator fails
                    validator_rules[i].validation_strategy = ValidationStrategy.LLM_REASONING
                else:
                    if result.get("is_violated", False):
                        state.violations.append(result.get("violation", {}))
                        state.analysis_steps.append(f"‚ö° Validator violation: {validator_rules[i].description[:50]}...")
                    else:
                        state.analysis_steps.append(f"‚ö° Validator passed: {validator_rules[i].description[:50]}...")

                    # Track validator usage
                    validator_name = validator_rules[i].validator_name
                    if validator_name:
                        state.validator_usage[validator_name] = state.validator_usage.get(validator_name, 0) + 1

        validator_time = (time.time() - start_time) * 1000
        logger.info(f"‚ö° Validator evaluation completed in {validator_time:.2f}ms")

    except Exception as e:
        logger.error(f"‚ùå Error in validator evaluation: {e}")
        state.analysis_steps.append(f"Error in validator evaluation: {str(e)}")

    return state


async def execute_llm_fallback(state: EngineState) -> EngineState:
    """
    Execute LLM reasoning for complex rules or as fallback for validator failures.
    """
    start_time = time.time()

    try:
        llm_rules = [
            rd
            for rd in state.rule_descriptions
            if rd.validation_strategy in [ValidationStrategy.LLM_REASONING, ValidationStrategy.HYBRID]
        ]
        logger.info(f"üß† Executing {len(llm_rules)} LLM evaluations")

        if not llm_rules:
            logger.info("üß† No LLM rules to evaluate")
            return state

        # Execute LLM evaluations concurrently (with rate limiting)
        llm = ChatOpenAI(api_key=config.ai.api_key, model=config.ai.model, max_tokens=2000, temperature=0.1)

        llm_tasks = []
        for rule_desc in llm_rules:
            task = _execute_single_llm_evaluation(rule_desc, state.event_data, state.event_type, llm)
            llm_tasks.append(task)

        if llm_tasks:
            # Execute in batches to avoid overwhelming the LLM
            batch_size = 3
            for i in range(0, len(llm_tasks), batch_size):
                batch = llm_tasks[i : i + batch_size]
                results = await asyncio.gather(*batch, return_exceptions=True)

                # Process batch results
                for j, result in enumerate(results):
                    rule_desc = llm_rules[i + j]
                    if isinstance(result, Exception):
                        logger.error(f"‚ùå LLM evaluation failed for rule '{rule_desc.description[:50]}...': {result}")
                        state.analysis_steps.append(f"üß† LLM failed: {rule_desc.description[:50]}...")
                    else:
                        if result.get("is_violated", False):
                            state.violations.append(result.get("violation", {}))
                            state.analysis_steps.append(f"üß† LLM violation: {rule_desc.description[:50]}...")
                        else:
                            state.analysis_steps.append(f"üß† LLM passed: {rule_desc.description[:50]}...")

                # Small delay between batches
                if i + batch_size < len(llm_tasks):
                    await asyncio.sleep(0.5)

        # Track LLM usage
        state.llm_usage = len(llm_rules)

        llm_time = (time.time() - start_time) * 1000
        logger.info(f"üß† LLM evaluation completed in {llm_time:.2f}ms")

    except Exception as e:
        logger.error(f"‚ùå Error in LLM evaluation: {e}")
        state.analysis_steps.append(f"Error in LLM evaluation: {str(e)}")

    return state


async def _execute_single_validator(rule_desc: RuleDescription, event_data: dict[str, Any]) -> dict[str, Any]:
    """Execute a single validator evaluation."""
    start_time = time.time()

    try:
        validator = VALIDATOR_REGISTRY[rule_desc.validator_name]
        is_valid = await validator.validate(rule_desc.parameters, event_data)
        is_violated = not is_valid

        execution_time = (time.time() - start_time) * 1000

        if is_violated:
            # Generate dynamic "how to fix" message using LLM
            how_to_fix = await _generate_dynamic_how_to_fix(rule_desc, event_data, validator.name)

            violation = {
                "rule_description": rule_desc.description,
                "severity": rule_desc.severity,
                "message": f"Rule validation failed: {rule_desc.description}",
                "details": {
                    "validator_used": rule_desc.validator_name,
                    "parameters": rule_desc.parameters,
                    "validation_result": "failed",
                },
                "how_to_fix": how_to_fix,
                "docs_url": "",
                "validation_strategy": ValidationStrategy.VALIDATOR,
                "execution_time_ms": execution_time,
            }
            return {"is_violated": True, "violation": violation}
        else:
            return {"is_violated": False, "execution_time_ms": execution_time}

    except Exception as e:
        execution_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Validator error for rule '{rule_desc.description[:50]}...': {e}")
        return {"is_violated": False, "error": str(e), "execution_time_ms": execution_time}


async def _execute_single_llm_evaluation(
    rule_desc: RuleDescription, event_data: dict[str, Any], event_type: str, llm
) -> dict[str, Any]:
    """Execute a single LLM evaluation."""
    start_time = time.time()

    try:
        # Create evaluation prompt
        evaluation_prompt = create_llm_evaluation_prompt(rule_desc, event_data, event_type)
        messages = [SystemMessage(content=get_llm_evaluation_system_prompt()), HumanMessage(content=evaluation_prompt)]

        # Use structured output for reliable parsing
        structured_llm = llm.with_structured_output(LLMEvaluationResponse)
        evaluation_result = await structured_llm.ainvoke(messages)

        execution_time = (time.time() - start_time) * 1000

        if evaluation_result.is_violated:
            violation = {
                "rule_description": rule_desc.description,
                "severity": rule_desc.severity,
                "message": evaluation_result.message,
                "details": evaluation_result.details,
                "how_to_fix": evaluation_result.how_to_fix or "",
                "docs_url": "",
                "validation_strategy": ValidationStrategy.LLM_REASONING,
                "execution_time_ms": execution_time,
            }
            return {"is_violated": True, "violation": violation}
        else:
            return {"is_violated": False, "execution_time_ms": execution_time}

    except Exception as e:
        execution_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå LLM evaluation error for rule '{rule_desc.description[:50]}...': {e}")
        return {"is_violated": False, "error": str(e), "execution_time_ms": execution_time}


async def _generate_dynamic_how_to_fix(
    rule_desc: RuleDescription, event_data: dict[str, Any], validator_name: str
) -> str:
    """Generate dynamic 'how to fix' message using LLM."""

    try:
        llm = get_chat_model(max_tokens=1000, temperature=0.1)

        # Create prompt for how to fix generation
        how_to_fix_prompt = create_how_to_fix_prompt(rule_desc, event_data, validator_name)
        messages = [
            SystemMessage(
                content="You are an expert at providing actionable guidance for fixing GitHub rule violations."
            ),
            HumanMessage(content=how_to_fix_prompt),
        ]

        # Use structured output for reliable parsing
        structured_llm = llm.with_structured_output(HowToFixResponse)
        how_to_fix_result = await structured_llm.ainvoke(messages)

        return how_to_fix_result.how_to_fix

    except Exception as e:
        logger.error(f"‚ùå Error generating how to fix message: {e}")
        # Fallback to generic message
        return f"Review and address the requirements for rule: {rule_desc.description}"


async def validate_violations(state: EngineState) -> EngineState:
    """Validate and format violations for output."""
    logger.info(f"üîß Violations validated: {len(state.violations)} violations")
    return state
